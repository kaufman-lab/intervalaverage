---
title: "Technical overview of the intervalaverage function"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{intervalaverage-technicaloverview}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup,echo=FALSE,message=FALSE}
library(intervalaverage)
```

## Motivation

The interval average function was born out of the need to take values measured 
over a set of intervals and time-weight average those values into a different set
of intervals. Algorithmically, the simplest approach to deal with this problem
is just to expand interval data into repeated values over intervals that are all
the same length (e.g. the value of `8` measured over the interval `[3,5]` becomes
the value of 8 measured over `[3,3]` and `[4,4]` and `[5,5]`) but this is incredibly
memory intensive.  Instead a weighting approach is necessary. 

I was not able to find a package which accomplished this task, much less
a package which accomplished it quickly and with minimal memory overhead.


## This vignette
This vignette describes some of the decision-making behind how the intervalaverage function was written.



## A brief description of the intervalaverage algorithm (with a focus on explaining why seemingly convoluted choices were made in the function body)

The `intervalaverage` function starts by merging periods in y to periods in x by
 interval variables and possible grouping variables using foverlaps. 
Technically speaking `intervalaverage(x,y, interval_vars,group_vars)`
performs a right join on group_vars and non-equi joining (for partial overlaps)
on interval_vars. The function then calculates the length of each joined interval
intersection (this is defined as the dur column in the function body).
  The average value of the value_vars is calculated over each interval
in y as `sum(value_vars*dur)/sum(temp_nobs_vars)`. For example,
to calculate the weighted average of a value over 
an interval of length 4 where a value of 8 was observed for 3 days and a value of
2 was observed for 1 day, we could take `8*3/4 + 2*1/4` or `(1/4)*(8*3+2*1)`.

The `intervalaverage` uses the latter equation due to the fact that data.table's 
group by arithmatic is highly optimized but only for specific calls.
data.table's gforce  efficeintly does group-by operations directly in C rather 
than in R. Since data.table's ability to detect an optimized group-by function 
call is limited, using gforce for an abritrary number of value_vars columns
requires use of some nonstandard evaluation. In this context, 
the non-standard evaluation is constructing a data.table call (ie a "macro")
 via paste and calling it with `eval(parse(text=<>))` (which has been defined 
 as the `EVAL` function as an alias to avoid writing that multiple times). 

An additional consideration is that since the function is written to accept 
data.tables with arbitrary columns, and in the joined data.table z there could 
be naming conflicts between user-supplied columns and constructed columns, all 
user-supplied columns are renamed to placeholder names until the end of the 
function. This avoids potential issues with naming conflicts with *temporary*
 internal columns that are not included in the output. However, output columns
 (`xduration`, `yduration`, `xminstart`, `xmaxend`) are reserved column names and can't
 exist as columns that need to end up in z (specifically, they can't be in group_vars, 
 interval_vars, or value_vars).  Since only `group_vars`,`interval_vars`, and `value_vars`
 go into the `foverlaps` merge, columns named `xduration`, `yduration`, `xminstart`, and
 `xmaxend` are allowed to be other columns of `x` or `y` as long as they're not `group_vars`,
 `value_vars`, or `interval_vars`.

## Memory usage: why it's not as good as it could be and how that might be fixable

Memory: 
The headline here is that if you run into memory issues you can always 
do a for loop iterating over group vars. There's an example of this at the end of 
the corresponding unit test .R file.

In greater detail: 
This function unfortunately does create a large allocation 
(the intermediate joined table `z`). Ideally the function would be written to avoid
 this large allocation by doing an immediate `by=.EACHI` 
 (using an `x[y]` non-equi join instead of `foverlaps`).
 
 Unfortunately I've tested this approach and while it avoids the large memory allocation, 
 it's much slower due to pmin/pmax (which are used to calculate interval intersects)
 needing to be calculated within groups defined by `.EACHI`.
 If foverlaps directly produced the equivalents of the `interval_start`/`interval_end`
 variables (ie the interval intersects),
 and if `foverlaps` supported immediate group-by operations (similar to `by=.EACHI`)
 then this function could be
 memory-efficient and relatively fast. That approach might still be slower than 
 the current approach since GFORCE isn't supported for by-without-by/.EACHI joins.
 Conceptually by-without-by creates each group and immediately summarizes that group
 before iterating to the next group. This saves memory because it never allocates
 the whole table, but may not be as well optimized as the GFORCE group-by operations.
 
 Still, there might be an eventual possibility
 for trading some speed for avoing the large memory allocation if `foverlaps` calculates
 intersect lengths.
 
 
 
